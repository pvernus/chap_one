---
title: 'Magnitude'
format: html
editor: visual
---

```{r load_pckg, include=FALSE, cache=FALSE}

source(here::here('scripts', 'library.R'))
```

# Hazard severity (S)

## Extreme weather events

### Wet

```{r import_wet}

# define the directory
data_path <- "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\source\\treatment\\era5\\rainfall\\wet"

# define the file pattern
files <- list.files(data_path, full.names = TRUE)

# import and arrange dta files
data_list <- lapply(files, function(file) {
  # read file
  data <- haven::read_dta(file)
  file_name <- basename(file)
  # rename variable
  names(data) <- ifelse(grepl("^p", names(data)), "value", names(data))
   # create indice variable
  if (grepl("rx1d", file_name)) {
    data$indice <- "rx1d"
  } else if (grepl("rx5d", file_name)) {
    data$indice <- "rx5d"
  }
  # create percentile variable
  if (grepl("p90", file_name)) {
    data$percentile <- "p90"
  } else if (grepl("p95", file_name)) {
    data$percentile <- "p95"
  } else if (grepl("p99", file_name)) {
    data$percentile <- "p99"
  }
  
  return(data)
})

# rename list objects
names(data_list) <- sapply(basename(files), function(x) {
  x <- sub("^geo_id_", "", x)
  sub("\\.dta$", "", x)
})

# combine list elements
data_wet <- do.call(rbind, data_list)

## remove useless objects
rm(data_list)
```

Alternative dataset:

```{r import_wet_mswep}
#| eval: false # Don't evaluate this chunk

# define the directory
data_path <- "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\source\\treatment\\mswep\\wet"

# define the file pattern
files <- list.files(data_path, full.names = TRUE)

# import and arrange dta files
data_list <- lapply(files, function(file) {
  # read file
  data <- haven::read_dta(file)
  file_name <- basename(file)
  # rename variable
  names(data) <- ifelse(grepl("^p", names(data)), "value", names(data))
   # create indice variable
  if (grepl("rx1d", file_name)) {
    data$indice <- "rx1d"
  } else if (grepl("rx5d", file_name)) {
    data$indice <- "rx5d"
  }
  # create percentile variable
  if (grepl("p90", file_name)) {
    data$percentile <- "p90"
  } else if (grepl("p95", file_name)) {
    data$percentile <- "p95"
  } else if (grepl("p99", file_name)) {
    data$percentile <- "p99"
  }
  
  return(data)
})

# rename list objects
names(data_list) <- sapply(basename(files), function(x) {
  x <- sub("^geo_id_", "", x)
  sub("\\.dta$", "", x)
})

# combine list elements
data_wet <- do.call(rbind, data_list)

## remove useless objects
rm(data_list)
```

### Dry

```{r import_dry}

# define the directory
data_path <- "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\source\\treatment\\era5\\rainfall\\dry"

# define the file pattern
files <- list.files(data_path, full.names = TRUE)

# import and arrange dta files
data_list <- lapply(files, function(file) {
  # read file
  data <- haven::read_dta(file)
  file_name <- basename(file)
  # rename variable
  names(data) <- ifelse(grepl("^p", names(data)), "value", names(data))  
   # create indice variable
  if (grepl("rx90d", file_name)) {
    data$indice <- "rx90d"
  } else if (grepl("rx180d", file_name)) {
    data$indice <- "rx180d"
  }
  # create percentile variable
  if (grepl("p10", file_name)) {
    data$percentile <- "p10"
  } else if (grepl("p5", file_name)) {
    data$percentile <- "p5"
  } else if (grepl("p1", file_name)) {
    data$percentile <- "p1"
  }

  return(data)
})

# rename list objects
names(data_list) <- sapply(basename(files), function(x) {
  x <- sub("^geo_id_", "", x)
  sub("\\.dta$", "", x)
})

# combine list elements
data_dry <- do.call(rbind, data_list)

## remove useless objects
rm(data_list)
```

Alternative dataset:

```{r import_dry_mswep}
#| eval: false # Don't evaluate this chunk

# define the directory
data_path <- "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\source\\treatment\\mswep\\dry"

# define the file pattern
files <- list.files(data_path, full.names = TRUE)

# import and arrange dta files
data_list <- lapply(files, function(file) {
  # read file
  data <- haven::read_dta(file)
  file_name <- basename(file)
  # rename variable
  names(data) <- ifelse(grepl("^p", names(data)), "value", names(data))  
   # create indice variable
  if (grepl("rx90d", file_name)) {
    data$indice <- "rx90d"
  } else if (grepl("rx180d", file_name)) {
    data$indice <- "rx180d"
  }
  # create percentile variable
  if (grepl("p10", file_name)) {
    data$percentile <- "p10"
  } else if (grepl("p5", file_name)) {
    data$percentile <- "p5"
  } else if (grepl("p1", file_name)) {
    data$percentile <- "p1"
  }

  return(data)
})

# rename list objects
names(data_list) <- sapply(basename(files), function(x) {
  x <- sub("^geo_id_", "", x)
  sub("\\.dta$", "", x)
})

# combine list elements
data_dry <- do.call(rbind, data_list)

## remove useless objects
rm(data_list)
```

### Hot

```{r import_hot}

# define the directory
data_path <- "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\source\\treatment\\era5\\temperature"

# define the file pattern
files <- list.files(data_path, full.names = TRUE)

# import and arrange dta files
data_list <- lapply(files, function(file) {
  # read file
  data <- haven::read_dta(file)
  file_name <- basename(file)
  # rename variable
  names(data) <- ifelse(grepl("^p", names(data)), "value", names(data))  
   # create indice variable
  if (grepl("tmax1d", file_name)) {
    data$indice <- "tmax1d"
  } else if (grepl("tmax5d", file_name)) {
    data$indice <- "tmax5d"
  }
  # create percentile variable
  if (grepl("p90", file_name)) {
    data$percentile <- "p90"
  } else if (grepl("p95", file_name)) {
    data$percentile <- "p95"
  } else if (grepl("p99", file_name)) {
    data$percentile <- "p99"
  }

  return(data)
})

# rename list objects
names(data_list) <- sapply(basename(files), function(x) {
  x <- sub("^geo_id_", "", x)
  sub("\\.dta$", "", x)
})

# combine list elements
data_hot <- do.call(rbind, data_list)

## remove useless objects
rm(data_list)
```

### Wind

```{r import_wind}

# define the directory
data_path <- "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\source\\treatment\\era5\\wind"

# define the file pattern
files <- list.files(data_path, full.names = TRUE)

# import and arrange dta files
data_list <- lapply(files, function(file) {
  # read file
  data <- haven::read_dta(file)
  file_name <- basename(file)
  # rename variable
  names(data) <- ifelse(grepl("^p", names(data)), "value", names(data))  
   # create indice variable
  data$indice <- "wmean1d"
  # create percentile variable
  if (grepl("p90", file_name)) {
    data$percentile <- "p90"
  } else if (grepl("p95", file_name)) {
    data$percentile <- "p95"
  } else if (grepl("p99", file_name)) {
    data$percentile <- "p99"
  }
  
  return(data)
})

# rename list objects
names(data_list) <- sapply(basename(files), function(x) {
  x <- sub("^geo_id_", "", x)
  sub("\\.dta$", "", x)
})

# combine list elements
data_wind <- do.call(rbind, data_list)

## remove useless objects
rm(data_list)
```

### All

```{r data_clim}

# create list
list_clim <- list(data_wet, data_dry, data_hot, data_wind)
# combine list elements
data_clim <- do.call(rbind, list_clim)

# create variables 
data_clim <- data_clim |> 
  mutate(
    threshold = case_when(
      percentile == "p1" | percentile == "p99" ~ 3.65,
      percentile == "p5" | percentile == "p95" ~ 18.25,
      percentile == "p10" | percentile == "p90" ~ 36.5),
    threshold_med = threshold * 1.5,
    threshold_large = threshold * 2,
    severe = ifelse(value >= threshold, 1, 0),
    severe_med = ifelse(value >= threshold_med, 1, 0),
    severe_large = ifelse(value >= threshold_large, 1, 0),
    year = as.double(year),
    percentile = factor(percentile, levels=c("p1", "p5", "p10", "p99", "p95", "p90"))
    )

# modify labels
labelled::var_label(data_clim$value) <- "(mean)"

# summary
summary(data_clim)
```
```{r}
# save
save(data_wet, data_dry, data_hot, data_wind, data_clim,
     file = "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\data\\treatment\\data_clim.RData")
```

```{r}

## facet by indice
# mean value
summarize(data_clim,
          mean_value = mean(value),
          .by = c(year, indice)) |> 
  filter(year <= 2018) |> 
  ggplot(aes(x = year, y = mean_value)) +
  geom_line() +
  geom_smooth(method=lm) +
  facet_wrap(~indice) +
  theme_light() +
  labs(
    x = "",
    y = "Mean days > threshold"
  )

# mean extreme
# Define the color palette
color_palette <- c("p1" = "#E91300", "p99" = "#E91300",
                   "p5" = "#FF912F", "p95" = "#FF912F",
                   "p10" = "#FFD17A", "p90" = "#FFD17A")
# plot
summarize(data_clim,
          mean_extr = mean(severe),
          .by = c(year, indice, percentile)) |> 
  filter(year <= 2018) |> 
  ggplot(aes(x = year, y = mean_extr, color = percentile)) +
  geom_line() +
  geom_smooth(method=lm, se = FALSE) +
  scale_color_manual(values = color_palette) +
  facet_wrap(~indice) +
  theme_light() +
  labs(
    x = "",
    y = "Proportion of extreme events (days > threshold)"
  )

```

## Disaster events

```{r load}

# Load datasets
load(file = "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\data\\treatment\\emdat\\data_emdat.RData") # EM-DAT
load(file = "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\data\\treatment\\data_gdis.RData") # GDIS
```

### EM-DAT

```{r clean_emdat}

## EMDAT
data_emdat_short <- data_emdat |> 
  select(dis_no, classification_key, disaster_type, iso, country, location, start_year, end_year, last_update) |> 
  filter(start_year %in% 2000:2018) |> # time period
  filter(grepl('nat-hyd-flo|nat-cli-dro|nat-met-sto|nat-met-ext-hea', classification_key)) |> # disaster type
  mutate(disasterno = stringr::str_sub(dis_no, end = -5)) |> 
  mutate(iso3c = countrycode(sourcevar = country,
                             origin = 'country.name',
                             destination = 'iso3c'
  )) |> 
  mutate(iso3c = case_when(
# NOTE: Canary Islands
    country == "Canary Islands" ~ "ESP",
# NOTE: Serbia Montenegro (iso==SCG)
# In variable 'location', we can find the name of one of the two countries at the end of the character string. I assume that it provides the true location.
    iso == "SCG" & grepl("Serbia)$", str_squish(location)) ~ "SRB",
    iso == "SCG" & grepl("Montenegro)$", str_squish(location)) ~ "MNE",
# NOTE: Kosovo
# Some obs. are located in Kosovo in GDIS, but in Serbia in EMDAT
# Kosovo is included in OECD CRS since 2009 (declaration of independence in 2008)
# Rule: if year < 2009 => Serbia (SRB) & if year >= 2009 => Kosovo (XKX)
# In EMDAT, two obs. don't apply to this rule
    dis_no %in% c("2013-0379-SRB", "2016-0446-SRB") ~ "XKX",
# NOTE South Sudan: 
# South Sudan is included in OECD CRS since 2011 (independent State in 2011)
# Rule: if year < 2011 => Sudan (SDN) & if year >= 2011 => South Sudan (SSD)
    country=='South Sudan' & start_year >= 2011 ~ 'SSD',
    country=='South Sudan' & start_year < 2011 ~ 'SDN',
    .default = iso3c
         )) |> 
# NOTE: duplicated obs. dis_n=="2010-0579" with two different location
# dis_no == "2010-0579-SDN": entry_date==2011; country=="Sudan") / 
# dis_no == "2010-0579-SSD": entry_date==2016; country=="South Sudan"
# I keep the observation which follows the rule set above  
  filter(dis_no != "2010-0579-SSD")

```

### GDIS

```{r clean_gdis}

## GDIS
data_gdis_short <- data_gdis |> 
  select(disasterno, dis_id = id, geo_id, disastertype, iso3, country) |> 
  mutate(year = as.numeric(substr(data_gdis$disasterno, 1, 4))) |> # time period
  filter(year >= 2000) |> 
  mutate(disastertype = trimws(disastertype)) |> # disaster type
  filter(disastertype %in% c('flood', 'drought', 'storm', 'extreme temperature')) |> 
  mutate(iso3c = countrycode(sourcevar = country,
                             origin = 'country.name',
                             destination = 'iso3c'
  )) |> 
  mutate(iso3c = case_when(
# Micronesia    
    country=='Micronesia' ~ 'FSM',
# NOTE Kosovo: same rule as for EMDAT
    country=='Kosovo' & year >= 2009 ~ 'XKX',
    country=='Kosovo' & year < 2009 ~ 'SRB',
# NOTE South Sudan: same rule as for EMDAT
    country=='South Sudan' & year >= 2011 ~ 'SSD',
    country=='South Sudan' & year < 2011 ~ 'SDN',
    .default = iso3c
    )) |> 
# NOTE: UKR/Crimea:
# disasterno=='2002-0482': three obs. in GDIS incl. one in Crimea/UKR (geo_id==22653) != RUS in EMDAT
# For now, I remove the UKR obs. geo_id
  filter(geo_id != 22653)

```

```{r compare_data}

## compare nb of events
length(unique(data_gdis_short$disasterno))
length(unique(data_emdat_short$disasterno))

## events in gdis, missing in emdat
anti_join(distinct(data_gdis_short, disasterno, disastertype),
          distinct(data_emdat_short, disasterno),
          by = "disasterno")
# NOTE: 163 events in GDIS w/out a match in EMDAT

## events in emdat, missing in gdis
anti_join(distinct(data_emdat_short, disasterno, disaster_type),
          distinct(data_gdis_short, disasterno),
          by = "disasterno")
# NOTE: 246 events in EMDAT w/out a match in GDIS

# Not sure why.
# Some events missing in EMDAT are due to *cold* extreme temperature events. GDIS does not differentiate between hot and cold extreme temperature.
# Note that EMDAT has been updated regularly *after* GDIS publication. So maybe missing events are due to deleted/added/modified event that have occurred following an update.
```

```{r compare_data_freq}

year_event_emdat = data_emdat_short |> 
  select(disaster_type, year = start_year, disasterno) |> 
  summarize(n = length(unique(disasterno)),
           .by = c(year, disaster_type)) |> 
  mutate(source = "emdat",
         disaster_type = tolower(disaster_type))

year_event_gdis = data_gdis_short |> 
  select(disaster_type = disastertype, year, disasterno) |> 
  summarize(n = length(unique(disasterno)),
           .by = c(year, disaster_type)) |> 
  mutate(source = "gdis")

year_event = rbind(year_event_emdat, year_event_gdis)

ggplot(year_event, aes(x = year, y = n, color = source)) +
  geom_line() + 
  facet_wrap(~ disaster_type, scales = "free_y") +
  theme_light() +
  labs(y = "Frequency")
```

```{r join_gdis_emdat}

emdat = data_emdat_short |> 
  select(disasterno, disaster_type, country, iso3c, start_year, end_year) |> 
  mutate(disaster_type = tolower(disaster_type))

gdis = data_gdis_short |> 
  select(disasterno, disaster_type = disastertype, country, iso3c, year, geo_id)

# merge
join_gdis_emdat <- left_join(gdis, emdat, 
                             by = c('disasterno', 'iso3c'), 
                             suffix = c('_gdis', '_emdat'))

# difference in disaster type classification
join_gdis_emdat |> 
  filter(disaster_type_gdis != disaster_type_emdat)
# NOTE: not an issue. Only one event in Italy (Storm/Flood)

# difference in year
join_gdis_emdat |> 
  filter(year < start_year | year > end_year)
# NOTE: five events where different specified year between EMDAT and GDIS. For all of them, GDIS event starts one year before same EMDAT event.

# Missing data
join_gdis_emdat |> 
  filter(is.na(disasterno))
```

```{r save join_gdis_emdat}

save(join_gdis_emdat, file = "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\data\\treatment\\join_gdis_emdat.RData")
```

## Merge weather-disaster events

```{r data_severity}

data_severity <- left_join(data_clim, join_gdis_emdat,
          by = "geo_id",
          suffix = c('', '_dis'))

# CHECK: object should have the same number of obs. as data_prec
length(unique((data_severity$geo_id)))
length(unique((data_clim$geo_id)))
```

```{r save data_severity}

save(data_severity, file = "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\data\\treatment\\data_severity.RData")
```

### Relative time to disaster year

```{r}

# Create variables
df <- data_severity |> 
  mutate(dis_dummy = ifelse(year == year_dis, 1, 0),
         dis_reltime = year - year_dis)
```

```{r}

# Define the list of disaster types
dis_type <- list("flood", "drought", "extreme temperature", "storm")
```

```{r}

# Share of 'extreme' years (value > threshold) 
df |> 
  filter(dis_reltime %in% c(-5:5)) |> 
  summarize(small = sum(severe)/length(severe), 
            medium = sum(severe_med)/length(severe_med),
            large = sum(severe_large)/length(severe_large),
            .by = c(dis_reltime, indice)
            ) |> 
  pivot_longer(cols = c("small", "medium", "large"), 
               names_to = "severe", values_to = "value") |> 
  mutate(severe = factor(severe, levels = c("small", "medium", "large"))) |> 
  ggplot(aes(x = factor(dis_reltime), y = value)) +
  geom_bar(stat = "identity") +
  gghighlight(dis_reltime==0, calculate_per_facet = T) +
  facet_wrap(~severe, scales="free_y") +
  theme_light()
  
```

```{r}

## share of year w/ severity > expected average
  
# define function
plot_prop_severe <- function(dis_type) {
  
  df %>%
    filter(disaster_type_gdis == dis_type) %>%
    filter(dis_reltime %in% c(-5:5)) %>%
    group_by(dis_reltime, indice, percentile) %>%
    summarize(prop = sum(severe)/length(severe), .groups = 'drop') %>%
    ungroup() %>%
    mutate(max = max(prop), .by = c(indice, percentile)) %>%
    mutate(fill_color = case_when(
      dis_reltime == 0 ~ 'grey30',
      prop == max ~ 'red',
      TRUE ~ 'grey'
    )) %>%
    mutate(percentile = factor(percentile, 
                               levels = c('p1', 'p5', 'p10', 'p99', 'p95', 'p90'))) %>%
  ggplot(aes(x = factor(dis_reltime), y = prop, fill = fill_color)) +
  geom_bar(position = position_dodge(width = 1), stat = "identity") +
  facet_wrap(indice ~ percentile, scales = "free", ncol = 3) +
  scale_fill_manual(values = c("grey30" = "grey30", "red" = "red", "grey" = "grey"),
                    name = "Legend",
                    labels = c("Other", "Disaster year", "Maximum value")) +
  theme_minimal() +
  labs(x = "Relative time to GDIS disaster year",
       y = "Share of extreme years")
}

lapply(dis_type, plot_prop_severe)
```

```{r}

## average severity measure

# define function
plot_mean_value <- function(dis_type) {
  
  df %>%
    filter(disaster_type_gdis == dis_type) %>%
    filter(dis_reltime %in% c(-5:5)) %>%
    group_by(dis_reltime, indice, percentile) %>%
    summarize(value = mean(value, na.rm = TRUE), .groups = 'drop') %>%
    ungroup() %>%
    mutate(max = max(value), .by = c(indice, percentile)) %>%
    mutate(fill_color = case_when(
      dis_reltime == 0 ~ 'grey30',
      value == max ~ 'red',
      TRUE ~ 'grey'
    )) %>%
    mutate(percentile = factor(percentile, 
                               levels = c('p1', 'p5', 'p10', 'p99', 'p95', 'p90'))) %>%
  ggplot(aes(x = factor(dis_reltime), y = value, fill = fill_color)) +
  geom_bar(position = position_dodge(width = 1), stat = "identity") +
  facet_wrap(indice ~ percentile, scales = "free", ncol = 3) +
  scale_fill_manual(values = c("grey30" = "grey30", "red" = "red", "grey" = "grey"),
                    name = "Legend",
                    labels = c("Other", "Disaster year", "Maximum value")) +
  theme_minimal() +
  labs(x = "Relative time to GDIS disaster year",
       y = "Average number of days (severity measure)")
}

lapply(dis_type, plot_mean_value)

```

```{r}
## boxplot

boxplot_value <- function(dis_type) {

df |> 
  filter(disaster_type_gdis ==  dis_type) |> 
  select(geo_id, value, indice, percentile, dis_reltime) |> 
  filter(dis_reltime %in% c(-5:5)) |> 
  ggplot(aes(x=factor(dis_reltime), y=value)) +
  geom_boxplot(outliers = F) +
  facet_wrap(indice ~ percentile, scales = "free", ncol = 3) +
  theme_minimal()
  
}

lapply(dis_type, boxplot_value)
```

# Hazard exposure (E)

## Landscan (pop)

```{r data_landscan}

# Define the folder path containing the .xlsx files
folder_path <- "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\source\\treatment\\landscan"

# List all files in the folder with the specified pattern
files <- list.files(path = folder_path, pattern = 'landscan_flood_\\d{4}\\.xlsx', full.names = TRUE)

# Function to read files and add year column
process_file <- function(file_path) {
  # Extract the year from the file name
  year <- str_extract(file_path, '(?<=landscan_flood_)\\d{4}') %>% as.numeric()
  
  # Validate the year range
  if (year %in% 2001:2022) {
    # Read the Excel file
    data <- read_excel(file_path, col_types = c('numeric', 'numeric', 'text', 
        'text', 'text', 'text', 'numeric', 
        'text', 'text', 'text', 'text', 'text', 
        'text', 'numeric', 'text', 'text', 
        'text', 'numeric', 'text', 'numeric', 
        'text', 'numeric', 'numeric', 'numeric'))
    # Add the year column
    data <- data |> 
      janitor::clean_names() |> 
      mutate(year = year) |> 
      rename_with(~ gsub('^pop_.*', 'pop', .), starts_with('pop_'))
    return(data)
  }
  return(NULL) # Return NULL for invalid years
}

# Process all files and combine the results
data_landscan <- files |> 
  lapply(process_file) |>       # Apply the processing function
  compact() |>                  # Remove NULLs from the list
  bind_rows()                   # Combine all data frames into one

```

```{r data_landscan_short}

## NOTE: measurement errors
# population estimations can vary widely from one year to another
# I compute trailing moving averages (k=3) to smooth pop estimations

data_landscan_short <- subset(data_landscan,
  select = c(geo_id, year, disasterno, pop)) |> 
  group_by(geo_id) |> 
  mutate(pop_tma = zoo::rollmean(pop, k = 3, fill = NA, align = "right")) |> 
# due to k=3, missing pop data in before 2003
  filter(year >= 2003) |> 
  pivot_longer(
    cols = starts_with("pop"),
    names_to = "pop",
    values_to = "pop_value") |> 
  ungroup() |> 
  arrange(geo_id)

head(data_landscan_short)
```

## Modis (land cover)

```{r data_modis}

# Define the folder path containing the .xlsx files
folder_path <- "C:\\Users\\pauvernu\\Seafile\\library\\chap_one_data\\source\\treatment\\modis"

# List all files in the folder with the specified pattern
files <- list.files(path = folder_path, pattern = '*.xlsx', full.names = TRUE)

# Function to read files and add year column
process_file <- function(file_path) {
  # Extract the year from the file name
  year <- str_extract(file_path, '(?<=modis_flood_)\\d{4}') |> as.numeric()
  
  # Validate the year range
  if (year %in% 2001:2022) {
    # Read the Excel file
    data <- read_excel(file_path)
    # Add the year column
    data <- data |> 
      janitor::clean_names() |> 
      mutate(year = year)
  }
  else {
    NULL # Return NULL for invalid years
  }
}

# Process all files and combine the results
data_modis <- files |> 
  lapply(process_file) |> # Apply the processing function
  compact() |> # Remove NULLs from the list (requires `purrr` library or alternative filtering)
  bind_rows() # Combine all data frames into one

```

> **Land cover classification**

Source: [link](https://modis-land.gsfc.nasa.gov/pdf/MCD12Q1_C6_Userguide04042018.pdf)

- Croplands: 12

- Cropland/Natural Vegetation Mosaics: 14

- Urban and Built-up Lands: 13

```{r data_modis_short}

data_modis_short <- data_modis |> 
  mutate(land_crop = c_12 + c_14,
         land_urb = c_13) |> 
  dplyr::select(geo_id, year, disasterno, starts_with("land")) |> 
  pivot_longer(
    cols = starts_with("land"),
    names_to = "land",
    values_to = "land_value") |> 
  arrange(geo_id)
    
head(data_modis_short)
```

# Hazard magnitude (M)

```{r}

## Compare length between landscan and modis datasets
# disasterno
length(unique(data_landscan_short$disasterno))
length(unique(data_modis_short$disasterno))
# geo_id
length(unique(data_landscan_short$geo_id))
length(unique(data_modis_short$geo_id))

# NOTE: LANDSCAN has less observations than MODIS because we removed years 2001-2002 after computing 3y-trailing moving averages of pop. estimates

## Landscan
# disasterno in landscan but missing in emdat/gdis
anti_join(distinct(data_landscan_short, disasterno), 
          distinct(data_severity, disasterno), 
          by = "disasterno")

# geo_id in landscan but missing in emdat/gdis
anti_join(distinct(data_landscan_short, geo_id, disasterno), 
          distinct(data_severity, geo_id, disasterno), 
          by = "geo_id")
# geo_id in emdat/gdis but missing in landscan
anti_join(distinct(data_severity, geo_id, disasterno), 
          distinct(data_landscan_short, geo_id, disasterno), 
          by = "geo_id")

## MODIS
# disasterno in modis but missing in emdat/gdis
anti_join(distinct(data_modis_short, disasterno), 
          distinct(data_severity, disasterno), 
          by = "disasterno")

# geo_id in modis but missing in emdat/gdis
anti_join(distinct(data_modis_short, geo_id, disasterno), 
          distinct(data_severity, geo_id, disasterno), 
          by = "geo_id")
# geo_id in emdat/gdis but missing in modis
anti_join(distinct(data_severity, geo_id, disasterno), 
          distinct(data_modis_short, geo_id, disasterno), 
          by = "geo_id")

# TO DO: check why we don't get the same number of distinct observations
# anti_join(data_severity, data_landscan, by = c("geo_id", "year"))
# anti_join(data_landscan, data_severity, by = c("geo_id", "year"))
```

```{r data_exposure}

# merge landscan (population) and modis (landuse)
data_exposure <- merge(data_landscan_short, data_modis_short, 
      by = c("geo_id", "disasterno", "year"))

```

```{r data_magnitude}

data_magnitude <- 
merge(data_severity, data_exposure, by = c("geo_id", "disasterno", "year"))
```



```{r distribution_value}

# Precipitation
data_magnitude |>  
  filter(year == year_dis) |>
  mutate(mean = mean(prec_value)) |> 
  ggplot(aes(x=prec_value)) +
  geom_density() +
  geom_vline(aes(xintercept = mean), linetype = "dashed", colour = "blue") +
  theme_light()

# Population
data_magnitude |>  
  filter(year == year_dis) |>
  mutate(mean = mean(pop_value)) |> 
  ggplot(aes(x=pop_value)) +
  geom_density() +
  geom_vline(aes(xintercept = mean), linetype = "dashed", colour = "blue") +
  theme_light()

# Land
data_magnitude |>  
  filter(year == year_dis) |>
  mutate(mean = mean(land_value)) |> 
  ggplot(aes(x=land_value)) +
  geom_density() +
  geom_vline(aes(xintercept = mean), linetype = "dashed", colour = "blue") +
  theme_light()
```

```{r treatment}

treatment <- left_join(data_unit, data_sd2_pop, by = c("iso3c", "year")) |> 
  mutate(across(starts_with("D_"), \(x) replace_na(x, 0)))
```

```{r}

save(treatment, file = here("data", "data_magnitude.RData"))
```

```{r}
rm(data_emdat, data_exposure, data_gdis, data_landscan, data_modis, key_emdat, GDIS_disasterlocations)
```
